{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Recognizer\n",
    "\n",
    "This notebook contains a trained model that, given an image of a sunspot, can detect whether skin cancer is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loaders\n",
    "\n",
    "Pull in the training/testing/validation images into a loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  2000\n",
      "Validation:  150\n",
      "Testing:  600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Common parameters/attributes\n",
    "n_epochs = 50\n",
    "batch_size = 20\n",
    "data_dir = 'D:/image_datasets/skin_cancer_images/'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Data transform will resize and images to 300 x 300, crop out the \n",
    "# center at as a 250x250 square and convert the image to a tensor.\n",
    "image_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.RandomResizedCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Load in data directories from each relevant folder and transform\n",
    "sc_data = {\n",
    "    'train': datasets.ImageFolder(data_dir + 'train', transform=image_transform),\n",
    "    'valid': datasets.ImageFolder(data_dir + 'valid', transform=image_transform),\n",
    "    'test': datasets.ImageFolder(data_dir + 'test', transform=image_transform)\n",
    "}\n",
    "\n",
    "# Prepare test/train/valid data loaders\n",
    "sc_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(sc_data['train'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    'valid': torch.utils.data.DataLoader(sc_data['valid'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(sc_data['test'], batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "}\n",
    "\n",
    "# Image counts\n",
    "print('Training: ', len(sc_data['train']))\n",
    "print('Validation: ', len(sc_data['valid']))\n",
    "print('Testing: ', len(sc_data['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Resnet50 Model\n",
    "\n",
    "Load in pretrained Resnet50 model and update last FCL for skincare classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Sequential(\n",
      "  (0): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=1000, out_features=3, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "## check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "## Add final FCL at the end of the model\n",
    "sc_model = nn.Sequential(\n",
    "    models.resnet50(pretrained=True),\n",
    "    nn.Linear(1000, 3)\n",
    ")\n",
    "\n",
    "## move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    print('Using cuda')\n",
    "    sc_model = sc_model.cuda()\n",
    "else:\n",
    "    print('Not using cuda')\n",
    "\n",
    "## Show model structure\n",
    "print(sc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer and Criterion\n",
    "\n",
    "Using SGD optimizer of CrossEntropyLoss criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "sc_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.0025\n",
    "sc_optimizer = optim.SGD(sc_model[1].parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Method\n",
    "\n",
    "A reusable function for training and saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When dealing with large images they may be truncated\n",
    "## and need special permission to load\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "    \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update validation loss\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch,\n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        # Save model if validation loss has decreased since last min\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.808634 \tValidation Loss: 0.895549\n",
      "Validation loss decreased (inf --> 0.895549).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.726156 \tValidation Loss: 0.820239\n",
      "Validation loss decreased (0.895549 --> 0.820239).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.702270 \tValidation Loss: 0.769043\n",
      "Validation loss decreased (0.820239 --> 0.769043).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.682880 \tValidation Loss: 0.730802\n",
      "Validation loss decreased (0.769043 --> 0.730802).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.668739 \tValidation Loss: 0.756836\n",
      "Epoch: 6 \tTraining Loss: 0.663418 \tValidation Loss: 0.825605\n",
      "Epoch: 7 \tTraining Loss: 0.673556 \tValidation Loss: 0.805150\n",
      "Epoch: 8 \tTraining Loss: 0.668890 \tValidation Loss: 0.716827\n",
      "Validation loss decreased (0.730802 --> 0.716827).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.647938 \tValidation Loss: 0.749728\n",
      "Epoch: 10 \tTraining Loss: 0.668333 \tValidation Loss: 0.737838\n",
      "Epoch: 11 \tTraining Loss: 0.669433 \tValidation Loss: 0.793697\n",
      "Epoch: 12 \tTraining Loss: 0.654913 \tValidation Loss: 0.787214\n",
      "Epoch: 13 \tTraining Loss: 0.663196 \tValidation Loss: 0.827995\n",
      "Epoch: 14 \tTraining Loss: 0.643928 \tValidation Loss: 0.735229\n",
      "Epoch: 15 \tTraining Loss: 0.650841 \tValidation Loss: 0.799548\n",
      "Epoch: 16 \tTraining Loss: 0.642779 \tValidation Loss: 0.797153\n",
      "Epoch: 17 \tTraining Loss: 0.650927 \tValidation Loss: 0.891472\n",
      "Epoch: 18 \tTraining Loss: 0.662432 \tValidation Loss: 0.730779\n",
      "Epoch: 19 \tTraining Loss: 0.658962 \tValidation Loss: 0.801642\n",
      "Epoch: 20 \tTraining Loss: 0.646057 \tValidation Loss: 0.782330\n",
      "Epoch: 21 \tTraining Loss: 0.642052 \tValidation Loss: 0.751524\n",
      "Epoch: 22 \tTraining Loss: 0.661581 \tValidation Loss: 0.751746\n",
      "Epoch: 23 \tTraining Loss: 0.651092 \tValidation Loss: 0.782064\n",
      "Epoch: 24 \tTraining Loss: 0.648998 \tValidation Loss: 0.787692\n",
      "Epoch: 25 \tTraining Loss: 0.628761 \tValidation Loss: 0.753343\n",
      "Epoch: 26 \tTraining Loss: 0.637899 \tValidation Loss: 0.713748\n",
      "Validation loss decreased (0.716827 --> 0.713748).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.640035 \tValidation Loss: 0.781198\n",
      "Epoch: 28 \tTraining Loss: 0.626681 \tValidation Loss: 0.789865\n",
      "Epoch: 29 \tTraining Loss: 0.642446 \tValidation Loss: 0.869027\n",
      "Epoch: 30 \tTraining Loss: 0.649069 \tValidation Loss: 0.761395\n",
      "Epoch: 31 \tTraining Loss: 0.638080 \tValidation Loss: 0.823179\n",
      "Epoch: 32 \tTraining Loss: 0.644677 \tValidation Loss: 0.813326\n",
      "Epoch: 33 \tTraining Loss: 0.636089 \tValidation Loss: 0.799185\n",
      "Epoch: 34 \tTraining Loss: 0.642022 \tValidation Loss: 0.742421\n",
      "Epoch: 35 \tTraining Loss: 0.637261 \tValidation Loss: 0.719137\n",
      "Epoch: 36 \tTraining Loss: 0.625794 \tValidation Loss: 0.734877\n",
      "Epoch: 37 \tTraining Loss: 0.641231 \tValidation Loss: 0.661264\n",
      "Validation loss decreased (0.713748 --> 0.661264).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.644231 \tValidation Loss: 0.731799\n",
      "Epoch: 39 \tTraining Loss: 0.643814 \tValidation Loss: 0.796971\n",
      "Epoch: 40 \tTraining Loss: 0.626703 \tValidation Loss: 0.835094\n",
      "Epoch: 41 \tTraining Loss: 0.622089 \tValidation Loss: 0.711937\n",
      "Epoch: 42 \tTraining Loss: 0.625037 \tValidation Loss: 0.768510\n",
      "Epoch: 43 \tTraining Loss: 0.635561 \tValidation Loss: 0.712992\n",
      "Epoch: 44 \tTraining Loss: 0.629671 \tValidation Loss: 0.799669\n",
      "Epoch: 45 \tTraining Loss: 0.626848 \tValidation Loss: 0.755752\n",
      "Epoch: 46 \tTraining Loss: 0.628588 \tValidation Loss: 0.642301\n",
      "Validation loss decreased (0.661264 --> 0.642301).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.640315 \tValidation Loss: 0.739023\n",
      "Epoch: 48 \tTraining Loss: 0.641489 \tValidation Loss: 0.709832\n",
      "Epoch: 49 \tTraining Loss: 0.638056 \tValidation Loss: 0.767144\n",
      "Epoch: 50 \tTraining Loss: 0.621673 \tValidation Loss: 0.751087\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "sc_model = train(n_epochs, sc_loaders, sc_model, sc_optimizer, sc_criterion, use_cuda, 'cancer_predictor_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Testing Method\n",
    "\n",
    "Method for testing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.797871\n",
      "\n",
      "\n",
      "Test Accuracy: 65% (391/600)\n"
     ]
    }
   ],
   "source": [
    "## Test the model\n",
    "test(sc_loaders, sc_model, sc_criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor Function\n",
    "\n",
    "Given an image path, it makes a prediction on the type of skin cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4907218  4.476143  -3.3212261]\n"
     ]
    }
   ],
   "source": [
    "## Image utils and pt Variable\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## Prediction method using Resnet50 model\n",
    "def sc_predict(img_path):\n",
    "    transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.RandomResizedCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    image = transform(Image.open(img_path))\n",
    "    image = image.unsqueeze(0)\n",
    "    prediction = False\n",
    "\n",
    "    if use_cuda:\n",
    "        prediction = sc_model.forward(Variable(image).cuda()).cpu()\n",
    "    else:\n",
    "        prediction = sc_model.forward(Variable(image))\n",
    "    \n",
    "    return prediction.data.numpy()[0]\n",
    "\n",
    "print(sc_predict('D:/image_datasets/skin_cancer_images/train/melanoma/ISIC_0000002.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to CSV\n",
    "\n",
    "\n",
    "### Columns:\n",
    "- Id - the file names of the test images (in the same order as the sample submission file)\n",
    "- task_1 - the model's predicted probability that the image (at the path in Id) depicts melanoma\n",
    "- task_2 - the model's predicted probability that the image (at the path in Id) depicts seborrheic keratosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [04:23<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('predictions.csv', mode='w') as csv_file:\n",
    "    ## Define columns\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=['Id', 'task_1', 'task_2'])\n",
    "    writer.writeheader()\n",
    "    \n",
    "    ## Evaluation mode\n",
    "    sc_model.eval()\n",
    "    \n",
    "    ## Paths\n",
    "    image_paths = glob(data_dir + 'test/*/*')\n",
    "\n",
    "    ## Process each file in test directory\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        \n",
    "        ## Get prediction\n",
    "        pred = sc_predict(image_paths[i])\n",
    "\n",
    "        ## Write results to csv\n",
    "        writer.writerow({\n",
    "            'Id': image_paths[i], \n",
    "            'task_1': pred[0], \n",
    "            'task_2': pred[2]\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
